{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f518f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = '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'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301270b-7a69-40f8-9934-f840499b6ae9",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0924a2-ce2d-44ba-a806-c1195720c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ad6fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88bdf13d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model,openai_api_key=openai.api_key)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db24677d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc3ef937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The answer to 1+1 is 2.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf3339a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The answer to 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Andrew, as you mentioned earlier.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2529400d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The answer to 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Andrew, as you mentioned earlier.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5018cb0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\\nHuman: What is 1+1?\\nAI: The answer to 1+1 is 2.\\nHuman: What is my name?\\nAI: Your name is Andrew, as you mentioned earlier.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14219b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a36e9905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61631b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2fdf9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca79256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "890a4497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98e9ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66eeccc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ea6233e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k defines the number of word that the model will remember\n",
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc4553fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a788403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4087bc87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model,openai_api_key=openai.api_key)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4faaa952",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb20ddaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer to 1+1 is 2.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "489b2194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have access to that information. Could you please tell me your name?\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2931b92",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb9020ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model,openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43582ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not import tiktoken python package. This is needed in order to calculate get_token_ids. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/openai.py:64\u001b[0m, in \u001b[0;36m_import_tiktoken\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/fanshutong/Documents/GitHub/DeepLearning/LLMs/LangChain/Study/2-Memory.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fanshutong/Documents/GitHub/DeepLearning/LLMs/LangChain/Study/2-Memory.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m memory \u001b[39m=\u001b[39m ConversationTokenBufferMemory(llm\u001b[39m=\u001b[39mllm, max_token_limit\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fanshutong/Documents/GitHub/DeepLearning/LLMs/LangChain/Study/2-Memory.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m memory\u001b[39m.\u001b[39;49msave_context({\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mAI is what?!\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fanshutong/Documents/GitHub/DeepLearning/LLMs/LangChain/Study/2-Memory.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                     {\u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mAmazing!\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fanshutong/Documents/GitHub/DeepLearning/LLMs/LangChain/Study/2-Memory.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m memory\u001b[39m.\u001b[39msave_context({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBackpropagation is what?\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fanshutong/Documents/GitHub/DeepLearning/LLMs/LangChain/Study/2-Memory.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                     {\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBeautiful!\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fanshutong/Documents/GitHub/DeepLearning/LLMs/LangChain/Study/2-Memory.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m memory\u001b[39m.\u001b[39msave_context({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mChatbots are what?\u001b[39m\u001b[39m\"\u001b[39m}, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fanshutong/Documents/GitHub/DeepLearning/LLMs/LangChain/Study/2-Memory.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                     {\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mCharming!\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/memory/token_buffer.py:53\u001b[0m, in \u001b[0;36mConversationTokenBufferMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[1;32m     52\u001b[0m buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39mmessages\n\u001b[0;32m---> 53\u001b[0m curr_buffer_length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mget_num_tokens_from_messages(buffer)\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m curr_buffer_length \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_token_limit:\n\u001b[1;32m     55\u001b[0m     pruned_memory \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/openai.py:585\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m7\u001b[39m:\n\u001b[1;32m    584\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_num_tokens_from_messages(messages)\n\u001b[0;32m--> 585\u001b[0m model, encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_encoding_model()\n\u001b[1;32m    586\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo-0301\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[39m# every message follows <im_start>{role/name}\\n{content}<im_end>\\n\u001b[39;00m\n\u001b[1;32m    588\u001b[0m     tokens_per_message \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/openai.py:548\u001b[0m, in \u001b[0;36mChatOpenAI._get_encoding_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_encoding_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, tiktoken\u001b[39m.\u001b[39mEncoding]:\n\u001b[0;32m--> 548\u001b[0m     tiktoken_ \u001b[39m=\u001b[39m _import_tiktoken()\n\u001b[1;32m    549\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiktoken_model_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiktoken_model_name\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/openai.py:66\u001b[0m, in \u001b[0;36m_import_tiktoken\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m tiktoken\n",
      "\u001b[0;31mValueError\u001b[0m: Could not import tiktoken python package. This is needed in order to calculate get_token_ids. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284288e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55d5d",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcf8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b238f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ecabe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728edba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a221b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb582617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6709b89-182e-4b7b-b811-1584dda20652",
   "metadata": {},
   "source": [
    "Reminder: Download your notebook to you local computer to save your work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
